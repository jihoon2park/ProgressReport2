#!/usr/bin/env python3
"""
Unified Data Sync Manager - í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì €
ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ëª¨ë“  ë°ì´í„°ë¥¼ ë™ê¸°í™”í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import sqlite3
import json
import os
import time
import threading
import schedule
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import logging

# í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ import (ìˆœí™˜ import ë°©ì§€)
try:
    from api_client import get_api_client, fetch_client_information
    from config import SITE_SERVERS
except ImportError as e:
    print(f"Warning: ì¼ë¶€ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    SITE_SERVERS = {}

# ì„ íƒì  import (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
try:
    from api_carearea import APICareArea
except ImportError:
    APICareArea = None

try:
    from api_eventtype import APIEventType
except ImportError:
    APIEventType = None

try:
    from api_incident import fetch_incidents_with_client_data
except ImportError:
    fetch_incidents_with_client_data = None

logger = logging.getLogger(__name__)

class UnifiedDataSyncManager:
    """í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì €"""
    
    def __init__(self, db_path='progress_report.db'):
        self.db_path = db_path
        self.sites = ['Parafield Gardens', 'Nerrilda', 'Ramsay', 'Yankalilla']
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.db_path):
            raise FileNotFoundError(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ {self.db_path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def get_db_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        conn = sqlite3.connect(self.db_path, timeout=30.0)
        conn.row_factory = sqlite3.Row
        return conn
    
    def update_sync_status(self, data_type: str, site: Optional[str] = None, 
                          status: str = 'success', records: int = 0, error: str = None):
        """ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            cursor.execute('PRAGMA busy_timeout = 30000')  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
            
            cursor.execute('''
                INSERT OR REPLACE INTO sync_status 
                (data_type, site, last_sync_time, sync_status, records_synced, error_message)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (data_type, site, datetime.now().isoformat(), status, records, error))
            
            conn.commit()
            
        except Exception as e:
            logger.error(f"ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        finally:
            if 'conn' in locals():
                conn.close()
    
    def sync_clients_data(self) -> Dict[str, Any]:
        """í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° ë™ê¸°í™”"""
        logger.info("ğŸ”„ í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° ë™ê¸°í™” ì‹œì‘")
        results = {'success': 0, 'failed': 0, 'total_changes': {'added': 0, 'updated': 0, 'removed': 0}}
        
        for site in self.sites:
            try:
                logger.info(f"  ğŸ“ {site} í´ë¼ì´ì–¸íŠ¸ ë™ê¸°í™” ì¤‘...")
                
                # APIì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                api_success, latest_clients = fetch_client_information(site)
                
                if not api_success:
                    logger.error(f"  âŒ {site} APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    self.update_sync_status('clients', site, 'failed', 0, 'API í˜¸ì¶œ ì‹¤íŒ¨')
                    results['failed'] += 1
                    continue
                
                # SQLite ìºì‹œ ì—…ë°ì´íŠ¸
                changes = self._update_clients_cache(site, latest_clients)
                results['total_changes']['added'] += changes['added']
                results['total_changes']['updated'] += changes['updated']
                results['total_changes']['removed'] += changes['removed']
                
                self.update_sync_status('clients', site, 'success', len(latest_clients))
                results['success'] += 1
                
                logger.info(f"  âœ… {site} ì™„ë£Œ: ì‹ ê·œ {changes['added']}ëª…, ì—…ë°ì´íŠ¸ {changes['updated']}ëª…, ì œê±° {changes['removed']}ëª…")
                
            except Exception as e:
                logger.error(f"  âŒ {site} í´ë¼ì´ì–¸íŠ¸ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
                self.update_sync_status('clients', site, 'failed', 0, str(e))
                results['failed'] += 1
        
        logger.info(f"ğŸ”„ í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {results['success']}/{len(self.sites)} ì‚¬ì´íŠ¸ ì„±ê³µ")
        return results
    
    def sync_care_areas_data(self) -> Dict[str, Any]:
        """ì¼€ì–´ ì˜ì—­ ë°ì´í„° ë™ê¸°í™”"""
        logger.info("ğŸ”„ ì¼€ì–´ ì˜ì—­ ë°ì´í„° ë™ê¸°í™” ì‹œì‘")
        
        if APICareArea is None:
            logger.warning("âš ï¸ APICareArea ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼€ì–´ ì˜ì—­ ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {'success': False, 'message': 'APICareArea ëª¨ë“ˆ ì—†ìŒ'}
        
        try:
            # APIì—ì„œ ì¼€ì–´ ì˜ì—­ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì²« ë²ˆì§¸ ì‚¬ì´íŠ¸ ì‚¬ìš©)
            api_carearea = APICareArea(self.sites[0])  # Parafield Gardens ì‚¬ìš©
            care_areas = api_carearea.get_care_area_information()
            
            if not care_areas:
                logger.error("âŒ ì¼€ì–´ ì˜ì—­ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.update_sync_status('carearea', None, 'failed', 0, 'API í˜¸ì¶œ ì‹¤íŒ¨')
                return {'success': False, 'message': 'API í˜¸ì¶œ ì‹¤íŒ¨'}
            
            # SQLite ìºì‹œ ì—…ë°ì´íŠ¸
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            for area in care_areas:
                cursor.execute('''
                    INSERT OR REPLACE INTO care_areas 
                    (id, description, is_archived, is_external, last_updated_date)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    area['Id'],
                    area['Description'],
                    area.get('IsArchived', False),
                    area.get('IsExternal', False),
                    area.get('LastUpdatedDate')
                ))
            
            conn.commit()
            conn.close()
            
            self.update_sync_status('carearea', None, 'success', len(care_areas))
            logger.info(f"âœ… ì¼€ì–´ ì˜ì—­ ë™ê¸°í™” ì™„ë£Œ: {len(care_areas)}ê°œ")
            
            return {'success': True, 'records': len(care_areas)}
            
        except Exception as e:
            logger.error(f"âŒ ì¼€ì–´ ì˜ì—­ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            self.update_sync_status('carearea', None, 'failed', 0, str(e))
            return {'success': False, 'message': str(e)}
    
    def sync_event_types_data(self) -> Dict[str, Any]:
        """ì´ë²¤íŠ¸ íƒ€ì… ë°ì´í„° ë™ê¸°í™”"""
        logger.info("ğŸ”„ ì´ë²¤íŠ¸ íƒ€ì… ë°ì´í„° ë™ê¸°í™” ì‹œì‘")
        
        if APIEventType is None:
            logger.warning("âš ï¸ APIEventType ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë²¤íŠ¸ íƒ€ì… ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {'success': False, 'message': 'APIEventType ëª¨ë“ˆ ì—†ìŒ'}
        
        try:
            # APIì—ì„œ ì´ë²¤íŠ¸ íƒ€ì… ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì²« ë²ˆì§¸ ì‚¬ì´íŠ¸ ì‚¬ìš©)
            api_eventtype = APIEventType(self.sites[0])  # Parafield Gardens ì‚¬ìš©
            event_types = api_eventtype.get_event_type_information()
            
            if not event_types:
                logger.error("âŒ ì´ë²¤íŠ¸ íƒ€ì… APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                self.update_sync_status('eventtype', None, 'failed', 0, 'API í˜¸ì¶œ ì‹¤íŒ¨')
                return {'success': False, 'message': 'API í˜¸ì¶œ ì‹¤íŒ¨'}
            
            # SQLite ìºì‹œ ì—…ë°ì´íŠ¸
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            for event_type in event_types:
                cursor.execute('''
                    INSERT OR REPLACE INTO event_types 
                    (id, description, color_argb, is_archived, is_external, last_updated_date)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    event_type['Id'],
                    event_type['Description'],
                    event_type.get('ColorArgb'),
                    event_type.get('IsArchived', False),
                    event_type.get('IsExternal', False),
                    event_type.get('LastUpdatedDate')
                ))
            
            conn.commit()
            conn.close()
            
            self.update_sync_status('eventtype', None, 'success', len(event_types))
            logger.info(f"âœ… ì´ë²¤íŠ¸ íƒ€ì… ë™ê¸°í™” ì™„ë£Œ: {len(event_types)}ê°œ")
            
            return {'success': True, 'records': len(event_types)}
            
        except Exception as e:
            logger.error(f"âŒ ì´ë²¤íŠ¸ íƒ€ì… ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            self.update_sync_status('eventtype', None, 'failed', 0, str(e))
            return {'success': False, 'message': str(e)}
    
    def sync_incidents_data(self) -> Dict[str, Any]:
        """ì¸ì‹œë˜íŠ¸ ë°ì´í„° ë™ê¸°í™”"""
        logger.info("ğŸ”„ ì¸ì‹œë˜íŠ¸ ë°ì´í„° ë™ê¸°í™” ì‹œì‘")
        results = {'success': 0, 'failed': 0, 'total_incidents': 0}
        
        if fetch_incidents_with_client_data is None:
            logger.warning("âš ï¸ fetch_incidents_with_client_data í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ì‹œë˜íŠ¸ ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {'success': False, 'message': 'fetch_incidents_with_client_data í•¨ìˆ˜ ì—†ìŒ'}
        
        # ìµœê·¼ 30ì¼ê°„ì˜ ì¸ì‹œë˜íŠ¸ ë°ì´í„° ë™ê¸°í™”
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        for site in self.sites:
            try:
                logger.info(f"  ğŸ“ {site} ì¸ì‹œë˜íŠ¸ ë™ê¸°í™” ì¤‘...")
                
                # APIì—ì„œ ì¸ì‹œë˜íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                incident_data = fetch_incidents_with_client_data(
                    site, 
                    start_date.strftime('%Y-%m-%d'), 
                    end_date.strftime('%Y-%m-%d')
                )
                
                if not incident_data or 'incidents' not in incident_data:
                    logger.error(f"  âŒ {site} ì¸ì‹œë˜íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    self.update_sync_status('incidents', site, 'failed', 0, 'API í˜¸ì¶œ ì‹¤íŒ¨')
                    results['failed'] += 1
                    continue
                
                incidents = incident_data['incidents']
                
                # SQLite ìºì‹œ ì—…ë°ì´íŠ¸
                conn = self.get_db_connection()
                cursor = conn.cursor()
                
                try:
                    # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    cursor.execute('PRAGMA busy_timeout = 30000')  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                    
                    for incident in incidents:
                        # incident_idê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                        incident_id = incident.get('IncidentId') or incident.get('Id') or incident.get('incident_id')
                        if not incident_id:
                            logger.warning(f"  âš ï¸ {site} ì¸ì‹œë˜íŠ¸ IDê°€ ì—†ì–´ì„œ ê±´ë„ˆëœ€: {incident}")
                            continue
                        
                        cursor.execute('''
                            INSERT OR REPLACE INTO incidents_cache 
                            (incident_id, client_id, client_name, incident_type, incident_date, 
                             description, severity, status, site, reported_by, last_synced)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            str(incident_id),  # ë¬¸ìì—´ë¡œ ë³€í™˜
                            incident.get('ClientId'),
                            incident.get('ClientName'),
                            incident.get('IncidentType'),
                            incident.get('IncidentDate'),
                            incident.get('Description'),
                            incident.get('Severity'),
                            incident.get('Status'),
                            site,
                            incident.get('ReportedBy'),
                            datetime.now().isoformat()
                        ))
                    
                    conn.commit()
                    
                except Exception as e:
                    conn.rollback()
                    raise e
                finally:
                    conn.close()
                
                self.update_sync_status('incidents', site, 'success', len(incidents))
                results['success'] += 1
                results['total_incidents'] += len(incidents)
                
                logger.info(f"  âœ… {site} ì™„ë£Œ: {len(incidents)}ê°œ ì¸ì‹œë˜íŠ¸")
                
            except Exception as e:
                logger.error(f"  âŒ {site} ì¸ì‹œë˜íŠ¸ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
                self.update_sync_status('incidents', site, 'failed', 0, str(e))
                results['failed'] += 1
        
        logger.info(f"ğŸ”„ ì¸ì‹œë˜íŠ¸ ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {results['success']}/{len(self.sites)} ì‚¬ì´íŠ¸ ì„±ê³µ, ì´ {results['total_incidents']}ê°œ")
        return results
    
    def _update_clients_cache(self, site: str, latest_clients: List[Dict]) -> Dict[str, int]:
        """SQLite í´ë¼ì´ì–¸íŠ¸ ìºì‹œ ì—…ë°ì´íŠ¸"""
        changes = {'added': 0, 'updated': 0, 'removed': 0, 'total': len(latest_clients)}
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            cursor.execute('''
                SELECT person_id, client_name, room_number, last_synced 
                FROM clients_cache 
                WHERE site = ? AND is_active = 1
            ''', (site,))
            
            existing_clients = {row['person_id']: dict(row) for row in cursor.fetchall()}
            
            # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì²˜ë¦¬
            current_person_ids = set()
            
            for client in latest_clients:
                person_id = (client.get('PersonId') or 
                           client.get('MainClientServiceId') or 
                           client.get('ClientRecordId'))
                
                if not person_id:
                    continue
                
                current_person_ids.add(person_id)
                
                client_name = (client.get('ClientName') or 
                             f"{client.get('FirstName', '')} {client.get('Surname', '')}".strip() or
                             client.get('PreferredName', 'Unknown'))
                
                if person_id in existing_clients:
                    # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì—…ë°ì´íŠ¸
                    cursor.execute('''
                        UPDATE clients_cache 
                        SET client_name = ?, preferred_name = ?, title = ?, first_name = ?,
                            middle_name = ?, surname = ?, gender = ?, birth_date = ?,
                            admission_date = ?, room_name = ?, room_number = ?, wing_name = ?,
                            location_id = ?, location_name = ?, main_client_service_id = ?,
                            original_person_id = ?, client_record_id = ?, last_synced = ?
                        WHERE person_id = ? AND site = ?
                    ''', (
                        client_name,
                        client.get('PreferredName'),
                        client.get('Title'),
                        client.get('FirstName'),
                        client.get('MiddleName'),
                        client.get('Surname') or client.get('LastName'),
                        client.get('Gender') or client.get('GenderDesc'),
                        client.get('BirthDate'),
                        client.get('AdmissionDate'),
                        client.get('RoomName'),
                        client.get('RoomNumber'),
                        client.get('WingName'),
                        client.get('LocationId'),
                        client.get('LocationName'),
                        client.get('MainClientServiceId'),
                        client.get('OriginalPersonId'),
                        client.get('ClientRecordId'),
                        datetime.now().isoformat(),
                        person_id,
                        site
                    ))
                    changes['updated'] += 1
                else:
                    # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€
                    cursor.execute('''
                        INSERT INTO clients_cache 
                        (person_id, client_name, preferred_name, title, first_name, 
                         middle_name, surname, gender, birth_date, admission_date,
                         room_name, room_number, wing_name, location_id, location_name,
                         main_client_service_id, original_person_id, client_record_id, 
                         site, last_synced, is_active)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        person_id,
                        client_name,
                        client.get('PreferredName'),
                        client.get('Title'),
                        client.get('FirstName'),
                        client.get('MiddleName'),
                        client.get('Surname') or client.get('LastName'),
                        client.get('Gender') or client.get('GenderDesc'),
                        client.get('BirthDate'),
                        client.get('AdmissionDate'),
                        client.get('RoomName'),
                        client.get('RoomNumber'),
                        client.get('WingName'),
                        client.get('LocationId'),
                        client.get('LocationName'),
                        client.get('MainClientServiceId'),
                        client.get('OriginalPersonId'),
                        client.get('ClientRecordId'),
                        site,
                        datetime.now().isoformat(),
                        True
                    ))
                    changes['added'] += 1
            
            # ì œê±°ëœ í´ë¼ì´ì–¸íŠ¸ ì²˜ë¦¬ (ë¹„í™œì„±í™”)
            removed_person_ids = set(existing_clients.keys()) - current_person_ids
            for person_id in removed_person_ids:
                cursor.execute('''
                    UPDATE clients_cache 
                    SET is_active = 0, last_synced = ?
                    WHERE person_id = ? AND site = ?
                ''', (datetime.now().isoformat(), person_id, site))
                changes['removed'] += 1
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()
        
        return changes
    
    def run_full_sync(self) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰"""
        logger.info("ğŸŒ… ë§¤ì¼ ìƒˆë²½ 3ì‹œ í†µí•© ë°ì´í„° ë™ê¸°í™” ì‹œì‘")
        start_time = datetime.now()
        
        results = {
            'start_time': start_time.isoformat(),
            'clients': {},
            'care_areas': {},
            'event_types': {},
            'incidents': {},
            'summary': {
                'total_success': 0,
                'total_failed': 0,
                'total_records': 0
            }
        }
        
        try:
            # 1. í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° ë™ê¸°í™”
            results['clients'] = self.sync_clients_data()
            results['summary']['total_success'] += results['clients']['success']
            results['summary']['total_failed'] += results['clients']['failed']
            results['summary']['total_records'] += sum(results['clients']['total_changes'].values())
            
            # 2. ì¼€ì–´ ì˜ì—­ ë°ì´í„° ë™ê¸°í™”
            results['care_areas'] = self.sync_care_areas_data()
            if results['care_areas']['success']:
                results['summary']['total_success'] += 1
                results['summary']['total_records'] += results['care_areas']['records']
            else:
                results['summary']['total_failed'] += 1
            
            # 3. ì´ë²¤íŠ¸ íƒ€ì… ë°ì´í„° ë™ê¸°í™”
            results['event_types'] = self.sync_event_types_data()
            if results['event_types']['success']:
                results['summary']['total_success'] += 1
                results['summary']['total_records'] += results['event_types']['records']
            else:
                results['summary']['total_failed'] += 1
            
            # 4. ì¸ì‹œë˜íŠ¸ ë°ì´í„° ë™ê¸°í™”
            results['incidents'] = self.sync_incidents_data()
            results['summary']['total_success'] += results['incidents']['success']
            results['summary']['total_failed'] += results['incidents']['failed']
            results['summary']['total_records'] += results['incidents']['total_incidents']
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            results['end_time'] = end_time.isoformat()
            results['duration_seconds'] = duration
            
            logger.info(f"ğŸŒ… í†µí•© ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {duration:.1f}ì´ˆ")
            logger.info(f"ğŸ“Š ê²°ê³¼: ì„±ê³µ {results['summary']['total_success']}ê°œ, ì‹¤íŒ¨ {results['summary']['total_failed']}ê°œ, ì´ {results['summary']['total_records']}ê°œ ë ˆì½”ë“œ")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ í†µí•© ë°ì´í„° ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            results['error'] = str(e)
            return results
    
    def start_daily_sync(self):
        """ë§¤ì¼ ìƒˆë²½ 3ì‹œ ë™ê¸°í™” ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        def daily_sync_job():
            """ë§¤ì¼ ìƒˆë²½ 3ì‹œ ë™ê¸°í™” ì‘ì—…"""
            logger.info("ğŸŒ… ë§¤ì¼ ìƒˆë²½ 3ì‹œ í†µí•© ë°ì´í„° ë™ê¸°í™” ì‹œì‘")
            results = self.run_full_sync()
            
            # ê²°ê³¼ ë¡œê¹…
            if 'error' in results:
                logger.error(f"âŒ ë™ê¸°í™” ì‹¤íŒ¨: {results['error']}")
            else:
                logger.info(f"âœ… ë™ê¸°í™” ì™„ë£Œ: {results['summary']['total_records']}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬")
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì • - ë§¤ì¼ ìƒˆë²½ 3ì‹œ
        schedule.every().day.at("03:00").do(daily_sync_job)
        
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ í™•ì¸
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
        sync_thread = threading.Thread(target=run_scheduler, daemon=True)
        sync_thread.start()
        
        logger.info("ğŸŒ… ë§¤ì¼ ìƒˆë²½ 3ì‹œ í†µí•© ë°ì´í„° ë™ê¸°í™” ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨")


# Flask ì•±ì—ì„œ ì‚¬ìš©í•  ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
unified_sync_manager = None

def get_unified_sync_manager():
    """í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    global unified_sync_manager
    if unified_sync_manager is None:
        unified_sync_manager = UnifiedDataSyncManager()
    return unified_sync_manager

def init_unified_sync():
    """Flask ì•± ì´ˆê¸°í™” ì‹œ í˜¸ì¶œ"""
    try:
        manager = get_unified_sync_manager()
        manager.start_daily_sync()
        logger.info("âœ… í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"âŒ í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False


# ëª…ë ¹ì¤„ì—ì„œ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ğŸŒ… í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    
    try:
        manager = UnifiedDataSyncManager()
        
        # ìˆ˜ë™ìœ¼ë¡œ ì „ì²´ ë™ê¸°í™” ì‹¤í–‰
        print("\nğŸ”„ ì „ì²´ ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰ ì¤‘...")
        results = manager.run_full_sync()
        
        print(f"\nğŸ“Š ë™ê¸°í™” ê²°ê³¼:")
        print(f"  - í´ë¼ì´ì–¸íŠ¸: {results['clients']['success']}/{len(manager.sites)} ì‚¬ì´íŠ¸ ì„±ê³µ")
        print(f"  - ì¼€ì–´ ì˜ì—­: {'ì„±ê³µ' if results['care_areas']['success'] else 'ì‹¤íŒ¨'}")
        print(f"  - ì´ë²¤íŠ¸ íƒ€ì…: {'ì„±ê³µ' if results['event_types']['success'] else 'ì‹¤íŒ¨'}")
        print(f"  - ì¸ì‹œë˜íŠ¸: {results['incidents']['success']}/{len(manager.sites)} ì‚¬ì´íŠ¸ ì„±ê³µ")
        print(f"  - ì´ ë ˆì½”ë“œ: {results['summary']['total_records']}ê°œ")
        print(f"  - ì†Œìš” ì‹œê°„: {results.get('duration_seconds', 0):.1f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def init_unifiedd_sync():
    """í†µí•© ë°ì´í„° ë™ê¸°í™” ì´ˆê¸°í™” í•¨ìˆ˜"""
    try:
        manager = UnifiedDataSyncManager()
        manager.start_background_sync()
        logger.info("âœ… í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    except Exception as e:
        logger.error(f"âŒ í†µí•© ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False
