#!/usr/bin/env python3
"""
Progress Report System - ê³ ê¸‰ ì„±ëŠ¥ ìµœì í™”
Week 3 - Day 3-4: ì„±ëŠ¥ ìµœì í™” ë° ê¸°ëŠ¥ ê°œì„ 
"""

import sqlite3
import json
import os
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict

class AdvancedOptimization:
    """ê³ ê¸‰ ì„±ëŠ¥ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self, db_path='progress_report.db'):
        self.db_path = db_path
        self.memory_cache = {}  # L1 ë©”ëª¨ë¦¬ ìºì‹œ
        self.cache_timestamps = {}  # ìºì‹œ íƒ€ì„ìŠ¤íƒ¬í”„
        self.cache_ttl = 300  # 5ë¶„ TTL
        
        if not os.path.exists(self.db_path):
            raise FileNotFoundError(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ {self.db_path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def get_db_connection(self):
        """ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        conn = sqlite3.connect(
            self.db_path, 
            timeout=30.0,
            check_same_thread=False  # ë©€í‹°ìŠ¤ë ˆë“œ ì§€ì›
        )
        conn.row_factory = sqlite3.Row
        
        # SQLite ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        conn.execute('PRAGMA journal_mode=WAL')  # Write-Ahead Logging
        conn.execute('PRAGMA synchronous=NORMAL')  # ë™ê¸°í™” ëª¨ë“œ
        conn.execute('PRAGMA cache_size=10000')  # ìºì‹œ í¬ê¸° ì¦ê°€
        conn.execute('PRAGMA temp_store=MEMORY')  # ì„ì‹œ í…Œì´ë¸”ì„ ë©”ëª¨ë¦¬ì—
        
        return conn
    
    # ===========================================
    # ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ
    # ===========================================
    
    def get_from_cache(self, cache_key: str) -> Optional[Any]:
        """L1 ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        if cache_key in self.memory_cache:
            # TTL í™•ì¸
            if cache_key in self.cache_timestamps:
                cache_time = self.cache_timestamps[cache_key]
                if datetime.now() - cache_time < timedelta(seconds=self.cache_ttl):
                    return self.memory_cache[cache_key]
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    del self.memory_cache[cache_key]
                    del self.cache_timestamps[cache_key]
        
        return None
    
    def set_cache(self, cache_key: str, data: Any):
        """L1 ë©”ëª¨ë¦¬ ìºì‹œì— ë°ì´í„° ì €ì¥"""
        self.memory_cache[cache_key] = data
        self.cache_timestamps[cache_key] = datetime.now()
    
    def clear_cache(self, pattern: str = None):
        """ìºì‹œ ì •ë¦¬"""
        if pattern:
            # íŒ¨í„´ì— ë§ëŠ” ìºì‹œë§Œ ì œê±°
            keys_to_remove = [key for key in self.memory_cache.keys() if pattern in key]
            for key in keys_to_remove:
                del self.memory_cache[key]
                if key in self.cache_timestamps:
                    del self.cache_timestamps[key]
        else:
            # ì „ì²´ ìºì‹œ ì œê±°
            self.memory_cache.clear()
            self.cache_timestamps.clear()
    
    # ===========================================
    # ìµœì í™”ëœ ë°ì´í„° ì¡°íšŒ í•¨ìˆ˜ë“¤
    # ===========================================
    
    def get_clients_optimized(self, site: str, search_term: str = None, 
                             use_cache: bool = True) -> List[Dict]:
        """ìµœì í™”ëœ í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ (ë‹¤ì¸µ ìºì‹±)"""
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"clients_{site}_{search_term or 'all'}"
        
        # L1 ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if use_cache:
            cached_data = self.get_from_cache(cache_key)
            if cached_data is not None:
                return cached_data
        
        # L2 SQLiteì—ì„œ ì¡°íšŒ
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            if search_term:
                # ê²€ìƒ‰ ì¿¼ë¦¬ (ì¸ë±ìŠ¤ í™œìš©)
                cursor.execute('''
                    SELECT * FROM clients_cache 
                    WHERE site = ? AND is_active = 1
                    AND (client_name LIKE ? OR preferred_name LIKE ? OR room_number LIKE ?)
                    ORDER BY 
                        CASE 
                            WHEN client_name LIKE ? THEN 1
                            WHEN preferred_name LIKE ? THEN 2
                            ELSE 3
                        END,
                        client_name
                ''', (site, f'%{search_term}%', f'%{search_term}%', f'%{search_term}%',
                      f'{search_term}%', f'{search_term}%'))
            else:
                # ì „ì²´ ì¡°íšŒ (ì¸ë±ìŠ¤ í™œìš©)
                cursor.execute('''
                    SELECT * FROM clients_cache 
                    WHERE site = ? AND is_active = 1
                    ORDER BY client_name
                ''', (site,))
            
            clients = [dict(row) for row in cursor.fetchall()]
            
            # L1 ìºì‹œì— ì €ì¥
            if use_cache:
                self.set_cache(cache_key, clients)
            
            return clients
            
        finally:
            conn.close()
    
    def get_dropdown_data_optimized(self) -> Dict[str, List[Dict]]:
        """ë“œë¡­ë‹¤ìš´ìš© ë°ì´í„° ìµœì í™” ì¡°íšŒ"""
        cache_key = "dropdown_data"
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        cached_data = self.get_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # í•œ ë²ˆì˜ ì—°ê²°ë¡œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
            cursor.execute('SELECT id, description FROM care_areas WHERE is_archived = 0 ORDER BY description')
            care_areas = [{'Id': row[0], 'Description': row[1]} for row in cursor.fetchall()]
            
            cursor.execute('SELECT id, description FROM event_types WHERE is_archived = 0 ORDER BY description')
            event_types = [{'Id': row[0], 'Description': row[1]} for row in cursor.fetchall()]
            
            dropdown_data = {
                'care_areas': care_areas,
                'event_types': event_types
            }
            
            # ìºì‹œì— ì €ì¥
            self.set_cache(cache_key, dropdown_data)
            
            return dropdown_data
            
        finally:
            conn.close()
    
    # ===========================================
    # ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥
    # ===========================================
    
    def search_clients_advanced(self, search_term: str, site: str = None, 
                               filters: Dict = None) -> List[Dict]:
        """ê³ ê¸‰ í´ë¼ì´ì–¸íŠ¸ ê²€ìƒ‰"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # ê¸°ë³¸ ì¿¼ë¦¬
            query = '''
                SELECT *, 
                       CASE 
                           WHEN client_name LIKE ? THEN 1
                           WHEN preferred_name LIKE ? THEN 2
                           WHEN room_number LIKE ? THEN 3
                           ELSE 4
                       END as relevance_score
                FROM clients_cache 
                WHERE is_active = 1
            '''
            
            params = [f'%{search_term}%', f'%{search_term}%', f'%{search_term}%']
            
            # ì¶”ê°€ ì¡°ê±´ë“¤
            conditions = []
            
            if site:
                conditions.append('site = ?')
                params.append(site)
            
            if filters:
                if filters.get('gender'):
                    conditions.append('gender = ?')
                    params.append(filters['gender'])
                
                if filters.get('wing'):
                    conditions.append('wing_name LIKE ?')
                    params.append(f'%{filters["wing"]}%')
                
                if filters.get('room_range'):
                    room_start, room_end = filters['room_range']
                    conditions.append('CAST(room_number AS INTEGER) BETWEEN ? AND ?')
                    params.extend([room_start, room_end])
            
            # ê²€ìƒ‰ ì¡°ê±´ ì¶”ê°€
            search_conditions = [
                'client_name LIKE ?',
                'preferred_name LIKE ?', 
                'room_number LIKE ?'
            ]
            
            if conditions:
                query += ' AND (' + ' OR '.join(search_conditions) + ') AND ' + ' AND '.join(conditions)
            else:
                query += ' AND (' + ' OR '.join(search_conditions) + ')'
            
            query += ' ORDER BY relevance_score, client_name'
            
            cursor.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]
            
        finally:
            conn.close()
    
    # ===========================================
    # í†µê³„ ë° ë¶„ì„ ê¸°ëŠ¥
    # ===========================================
    
    def get_client_statistics(self) -> Dict[str, Any]:
        """í´ë¼ì´ì–¸íŠ¸ í†µê³„ ë¶„ì„"""
        cache_key = "client_statistics"
        
        # ìºì‹œ í™•ì¸
        cached_stats = self.get_from_cache(cache_key)
        if cached_stats is not None:
            return cached_stats
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            stats = {}
            
            # ì‚¬ì´íŠ¸ë³„ í´ë¼ì´ì–¸íŠ¸ ìˆ˜
            cursor.execute('''
                SELECT site, COUNT(*) as count, 
                       COUNT(CASE WHEN gender = 'Male' THEN 1 END) as male_count,
                       COUNT(CASE WHEN gender = 'Female' THEN 1 END) as female_count
                FROM clients_cache 
                WHERE is_active = 1
                GROUP BY site
                ORDER BY site
            ''')
            
            site_stats = {}
            total_clients = 0
            
            for row in cursor.fetchall():
                site, count, male, female = row
                site_stats[site] = {
                    'total': count,
                    'male': male,
                    'female': female
                }
                total_clients += count
            
            stats['by_site'] = site_stats
            stats['total_clients'] = total_clients
            
            # ë°© ì ìœ ìœ¨ ë¶„ì„
            cursor.execute('''
                SELECT site, 
                       COUNT(CASE WHEN room_number IS NOT NULL AND room_number != '' THEN 1 END) as occupied_rooms,
                       COUNT(*) as total_clients
                FROM clients_cache 
                WHERE is_active = 1
                GROUP BY site
            ''')
            
            room_stats = {}
            for row in cursor.fetchall():
                site, occupied, total = row
                room_stats[site] = {
                    'occupied_rooms': occupied,
                    'total_clients': total,
                    'occupancy_rate': round((occupied / total * 100), 1) if total > 0 else 0
                }
            
            stats['room_occupancy'] = room_stats
            
            # ìµœê·¼ ë™ê¸°í™” ìƒíƒœ
            cursor.execute('''
                SELECT site, last_sync_time, records_synced
                FROM sync_status 
                WHERE data_type = 'clients'
                ORDER BY last_sync_time DESC
            ''')
            
            sync_stats = {}
            for row in cursor.fetchall():
                site, last_sync, records = row
                if last_sync:
                    sync_time = datetime.fromisoformat(last_sync)
                    age_minutes = int((datetime.now() - sync_time).total_seconds() / 60)
                    sync_stats[site] = {
                        'last_sync': last_sync,
                        'records': records,
                        'age_minutes': age_minutes,
                        'is_fresh': age_minutes < 30
                    }
            
            stats['sync_status'] = sync_stats
            
            # ìºì‹œì— ì €ì¥
            self.set_cache(cache_key, stats)
            
            return stats
            
        finally:
            conn.close()
    
    def get_usage_analytics(self, days: int = 7) -> Dict[str, Any]:
        """ì‚¬ìš© ë¶„ì„ (ìµœê·¼ Nì¼)"""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            analytics = {}
            
            # ì‚¬ìš©ìë³„ ì ‘ê·¼ í†µê³„
            cursor.execute('''
                SELECT username, role, COUNT(*) as access_count,
                       MAX(timestamp) as last_access
                FROM access_logs 
                WHERE timestamp > datetime('now', '-{} days')
                GROUP BY username, role
                ORDER BY access_count DESC
            '''.format(days))
            
            user_stats = []
            for row in cursor.fetchall():
                user_stats.append({
                    'username': row[0],
                    'role': row[1],
                    'access_count': row[2],
                    'last_access': row[3]
                })
            
            analytics['user_activity'] = user_stats
            
            # Progress Note ì‘ì„± í†µê³„
            cursor.execute('''
                SELECT site, COUNT(*) as note_count,
                       COUNT(DISTINCT username) as unique_users
                FROM progress_note_logs 
                WHERE timestamp > datetime('now', '-{} days')
                GROUP BY site
                ORDER BY note_count DESC
            '''.format(days))
            
            note_stats = []
            for row in cursor.fetchall():
                note_stats.append({
                    'site': row[0],
                    'note_count': row[1],
                    'unique_users': row[2]
                })
            
            analytics['progress_note_activity'] = note_stats
            
            # ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´
            cursor.execute('''
                SELECT strftime('%H', timestamp) as hour, COUNT(*) as count
                FROM access_logs 
                WHERE timestamp > datetime('now', '-{} days')
                GROUP BY hour
                ORDER BY hour
            '''.format(days))
            
            hourly_stats = {}
            for row in cursor.fetchall():
                hourly_stats[row[0]] = row[1]
            
            analytics['hourly_usage'] = hourly_stats
            
            return analytics
            
        finally:
            conn.close()
    
    # ===========================================
    # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
    # ===========================================
    
    def optimize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤í–‰"""
        print("=" * 60)
        print("ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤í–‰")
        print("=" * 60)
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        try:
            # 1. í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸
            print("\n1. í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸")
            print("-" * 40)
            
            start_time = time.time()
            cursor.execute('ANALYZE')
            analyze_time = (time.time() - start_time) * 1000
            print(f"  âœ“ ANALYZE ì™„ë£Œ: {analyze_time:.2f}ms")
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
            print("\n2. ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬")
            print("-" * 40)
            
            start_time = time.time()
            cursor.execute('VACUUM')
            vacuum_time = (time.time() - start_time) * 1000
            print(f"  âœ“ VACUUM ì™„ë£Œ: {vacuum_time:.2f}ms")
            
            # 3. ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  í™•ì¸
            print("\n3. ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  í™•ì¸")
            print("-" * 40)
            
            cursor.execute('''
                SELECT name, sql FROM sqlite_master 
                WHERE type = 'index' AND name NOT LIKE 'sqlite_%'
                ORDER BY name
            ''')
            
            indexes = cursor.fetchall()
            print(f"  âœ“ ì‚¬ìš©ì ì •ì˜ ì¸ë±ìŠ¤: {len(indexes)}ê°œ")
            
            for index in indexes:
                print(f"    - {index[0]}")
            
            # 4. ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
            print("\n4. ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´")
            print("-" * 40)
            
            db_size = os.path.getsize(self.db_path) / 1024 / 1024  # MB
            print(f"  ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {db_size:.2f} MB")
            
            # í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜
            tables = ['users', 'clients_cache', 'care_areas', 'event_types', 
                     'fcm_tokens', 'access_logs', 'progress_note_logs']
            
            for table in tables:
                cursor.execute(f'SELECT COUNT(*) FROM {table}')
                count = cursor.fetchone()[0]
                print(f"  {table}: {count:,}ê°œ")
            
        finally:
            conn.close()
    
    def create_additional_indexes(self):
        """ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)"""
        print("\nì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„±")
        print("-" * 40)
        
        conn = self.get_db_connection()
        cursor = conn.cursor()
        
        additional_indexes = [
            # ë³µí•© ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤
            ('idx_clients_search', 'clients_cache', '(site, client_name, preferred_name, room_number)'),
            ('idx_clients_active_site', 'clients_cache', '(is_active, site)'),
            
            # ë¡œê·¸ ë¶„ì„ì„ ìœ„í•œ ì¸ë±ìŠ¤
            ('idx_access_logs_user_time', 'access_logs', '(username, timestamp DESC)'),
            ('idx_progress_logs_site_time', 'progress_note_logs', '(site, timestamp DESC)'),
            
            # í†µê³„ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤
            ('idx_clients_gender_site', 'clients_cache', '(site, gender)'),
            ('idx_clients_room_site', 'clients_cache', '(site, room_number)'),
        ]
        
        try:
            for index_name, table_name, columns in additional_indexes:
                try:
                    cursor.execute(f'CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} {columns}')
                    print(f"  âœ“ {index_name} ìƒì„± ì™„ë£Œ")
                except sqlite3.Error as e:
                    print(f"  ! {index_name} ìƒì„± ì‹¤íŒ¨: {e}")
            
            conn.commit()
            print("  âœ… ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            
        finally:
            conn.close()
    
    # ===========================================
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    # ===========================================
    
    def run_performance_benchmark(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("\n" + "=" * 60)
        print("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)
        
        benchmark_results = {}
        
        # 1. í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ ë²¤ì¹˜ë§ˆí¬
        print("\n1. í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ ë²¤ì¹˜ë§ˆí¬")
        print("-" * 40)
        
        sites = ['Parafield Gardens', 'Nerrilda', 'Ramsay', 'Yankalilla']
        
        for site in sites:
            times = []
            for i in range(10):  # 10íšŒ ë°˜ë³µ
                start_time = time.time()
                clients = self.get_clients_optimized(site, use_cache=False)  # ìºì‹œ ë¹„í™œì„±í™”
                times.append((time.time() - start_time) * 1000)
            
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            
            benchmark_results[f'{site}_query'] = {
                'avg': avg_time,
                'min': min_time,
                'max': max_time,
                'count': len(clients) if 'clients' in locals() else 0
            }
            
            print(f"  {site}: í‰ê·  {avg_time:.2f}ms (ìµœì†Œ {min_time:.2f}ms, ìµœëŒ€ {max_time:.2f}ms)")
        
        # 2. ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        print("\n2. ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("-" * 40)
        
        search_terms = ['Smith', 'A', 'John', '1', '10']
        
        for term in search_terms:
            times = []
            for i in range(5):  # 5íšŒ ë°˜ë³µ
                start_time = time.time()
                results = self.search_clients_advanced(term)
                times.append((time.time() - start_time) * 1000)
            
            avg_time = sum(times) / len(times)
            result_count = len(results) if 'results' in locals() else 0
            
            benchmark_results[f'search_{term}'] = {
                'avg': avg_time,
                'count': result_count
            }
            
            print(f"  '{term}': {result_count}ëª…, í‰ê·  {avg_time:.2f}ms")
        
        # 3. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n3. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        # ì²« ë²ˆì§¸ í˜¸ì¶œ (DB ì¡°íšŒ)
        start_time = time.time()
        clients1 = self.get_clients_optimized('Parafield Gardens', use_cache=True)
        first_call_time = (time.time() - start_time) * 1000
        
        # ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ì‚¬ìš©)
        start_time = time.time()
        clients2 = self.get_clients_optimized('Parafield Gardens', use_cache=True)
        second_call_time = (time.time() - start_time) * 1000
        
        cache_improvement = first_call_time / second_call_time if second_call_time > 0 else 0
        
        print(f"  ì²« ë²ˆì§¸ í˜¸ì¶œ (DB): {first_call_time:.2f}ms")
        print(f"  ë‘ ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ): {second_call_time:.2f}ms")
        print(f"  ìºì‹œ ì„±ëŠ¥ í–¥ìƒ: {cache_improvement:.1f}ë°°")
        
        benchmark_results['cache_performance'] = {
            'db_time': first_call_time,
            'cache_time': second_call_time,
            'improvement': cache_improvement
        }
        
        return benchmark_results
    
    def generate_optimization_report(self):
        """ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 60)
        print("ìµœì í™” ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # í´ë¼ì´ì–¸íŠ¸ í†µê³„
        client_stats = self.get_client_statistics()
        
        print("\nğŸ“Š í´ë¼ì´ì–¸íŠ¸ í˜„í™©:")
        print(f"  ì „ì²´ í´ë¼ì´ì–¸íŠ¸: {client_stats['total_clients']:,}ëª…")
        
        for site, stats in client_stats['by_site'].items():
            print(f"  {site}: {stats['total']}ëª… (ë‚¨ì„± {stats['male']}, ì—¬ì„± {stats['female']})")
        
        print("\nğŸ  ë°© ì ìœ ìœ¨:")
        for site, stats in client_stats['room_occupancy'].items():
            print(f"  {site}: {stats['occupancy_rate']}% ({stats['occupied_rooms']}/{stats['total_clients']})")
        
        print("\nğŸ”„ ë™ê¸°í™” ìƒíƒœ:")
        for site, stats in client_stats['sync_status'].items():
            freshness = "ìµœì‹ " if stats['is_fresh'] else "ë§Œë£Œ"
            print(f"  {site}: {stats['records']}ëª…, {stats['age_minutes']}ë¶„ ì „ ({freshness})")
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        benchmark = self.run_performance_benchmark()
        
        print("\nğŸš€ ì„±ëŠ¥ ìš”ì•½:")
        avg_query_times = [result['avg'] for key, result in benchmark.items() 
                          if key.endswith('_query')]
        if avg_query_times:
            overall_avg = sum(avg_query_times) / len(avg_query_times)
            print(f"  í‰ê·  ì¿¼ë¦¬ ì‹œê°„: {overall_avg:.2f}ms")
        
        if 'cache_performance' in benchmark:
            cache_perf = benchmark['cache_performance']
            print(f"  ìºì‹œ ì„±ëŠ¥ í–¥ìƒ: {cache_perf['improvement']:.1f}ë°°")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        optimizer = AdvancedOptimization()
        
        # ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„±
        optimizer.create_additional_indexes()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
        optimizer.optimize_database()
        
        # ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±
        optimizer.generate_optimization_report()
        
        print("\nğŸ‰ Week 3 - Day 3-4 ìµœì í™” ì™„ë£Œ!")
        print("ë‹¤ìŒ ë‹¨ê³„: ìµœì¢… í…ŒìŠ¤íŠ¸ ë° ë°°í¬ ì¤€ë¹„")
        
    except Exception as e:
        print(f"\nâŒ ìµœì í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
