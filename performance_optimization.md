# Progress Report System - ì„±ëŠ¥ ìµœì í™” ì „ëµ

## ğŸ¯ ì„±ëŠ¥ ëª©í‘œ

| ì‘ì—… | í˜„ì¬ (JSON) | ëª©í‘œ (SQLite) | ê°œì„ ìœ¨ |
|------|-------------|---------------|--------|
| ì‚¬ìš©ì ë¡œê·¸ì¸ | 100ms | 20ms | **5x** |
| í´ë¼ì´ì–¸íŠ¸ ê²€ìƒ‰ | 500ms | 50ms | **10x** |
| ë“œë¡­ë‹¤ìš´ ë¡œë”© | 200ms | 20ms | **10x** |
| Progress Note ì €ì¥ | 300ms | 30ms | **10x** |
| ë¡œê·¸ ì¡°íšŒ | 1000ms | 100ms | **10x** |

## ğŸ”§ ìµœì í™” ê¸°ë²•

### 1. **ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”**

#### ì¸ë±ìŠ¤ ì „ëµ
```sql
-- ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´ë³„ ì¸ë±ìŠ¤
CREATE INDEX idx_clients_search ON clients_cache(site, client_name, room_number);
CREATE INDEX idx_logs_user_time ON access_logs(user_id, timestamp DESC);
CREATE INDEX idx_progress_notes_client_time ON progress_note_logs(client_id, timestamp DESC);

-- ë³µí•© ì¸ë±ìŠ¤ë¡œ ì»¤ë²„ë§ ì¸ë±ìŠ¤ í™œìš©
CREATE INDEX idx_clients_list ON clients_cache(site, is_active) 
    INCLUDE (client_name, preferred_name, room_number);
```

#### ì¿¼ë¦¬ ìµœì í™”
```python
# Before: ì „ì²´ ë°ì´í„° ë¡œë“œ í›„ í•„í„°ë§
def get_clients_old(site, room_filter=None):
    with open(f'{site}_client.json', 'r') as f:
        all_clients = json.load(f)
    
    if room_filter:
        return [c for c in all_clients if room_filter in c.get('room_number', '')]
    return all_clients

# After: DBì—ì„œ í•„í„°ë§ëœ ê²°ê³¼ë§Œ ì¡°íšŒ
def get_clients_new(site, room_filter=None):
    query = "SELECT * FROM clients_cache WHERE site = ? AND is_active = 1"
    params = [site]
    
    if room_filter:
        query += " AND room_number LIKE ?"
        params.append(f"%{room_filter}%")
    
    with get_db_connection() as conn:
        return conn.execute(query, params).fetchall()
```

### 2. **ìºì‹± ì „ëµ**

#### ë‹¤ì¸µ ìºì‹±
```python
class MultiLevelCache:
    def __init__(self):
        self.memory_cache = {}  # L1: ë©”ëª¨ë¦¬ ìºì‹œ
        self.db_cache = None    # L2: SQLite ìºì‹œ
        self.json_backup = None # L3: JSON ë°±ì—…
    
    def get_data(self, key):
        # L1: ë©”ëª¨ë¦¬ì—ì„œ í™•ì¸
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # L2: DBì—ì„œ í™•ì¸
        data = self.get_from_db(key)
        if data:
            self.memory_cache[key] = data  # L1ì— ìºì‹œ
            return data
        
        # L3: JSONì—ì„œ ë¡œë“œ
        data = self.get_from_json(key)
        if data:
            self.save_to_db(key, data)      # L2ì— ì €ì¥
            self.memory_cache[key] = data   # L1ì— ìºì‹œ
            return data
        
        return None
```

#### ìŠ¤ë§ˆíŠ¸ ìºì‹œ ë¬´íš¨í™”
```python
class SmartCacheInvalidation:
    def __init__(self):
        self.cache_dependencies = {
            'clients': ['progress_notes', 'incidents'],
            'care_areas': ['progress_notes'],
            'event_types': ['progress_notes']
        }
    
    def invalidate_cache(self, data_type):
        """ì—°ê´€ëœ ìºì‹œë“¤ë„ í•¨ê»˜ ë¬´íš¨í™”"""
        # ì§ì ‘ ë¬´íš¨í™”
        self.clear_cache(data_type)
        
        # ì˜ì¡´ì„±ì´ ìˆëŠ” ìºì‹œë“¤ë„ ë¬´íš¨í™”
        for dependent in self.cache_dependencies.get(data_type, []):
            self.clear_cache(dependent)
```

### 3. **ë¹„ë™ê¸° ì²˜ë¦¬**

#### ë°±ê·¸ë¼ìš´ë“œ ë™ê¸°í™”
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncDataSync:
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def sync_all_data(self):
        """ëª¨ë“  ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ë™ê¸°í™”"""
        tasks = [
            self.sync_clients('Parafield Gardens'),
            self.sync_clients('Nerrilda'),
            self.sync_clients('Ramsay'),
            self.sync_clients('Yankalilla'),
            self.sync_reference_data()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    async def sync_clients(self, site):
        """ê°œë³„ ì‚¬ì´íŠ¸ í´ë¼ì´ì–¸íŠ¸ ë™ê¸°í™”"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor, 
            self._sync_clients_sync, 
            site
        )
```

### 4. **ì—°ê²° í’€ë§**

#### SQLite ì—°ê²° ê´€ë¦¬
```python
import sqlite3
from contextlib import contextmanager
import threading

class ConnectionPool:
    def __init__(self, db_path, max_connections=10):
        self.db_path = db_path
        self.max_connections = max_connections
        self.pool = []
        self.lock = threading.Lock()
    
    @contextmanager
    def get_connection(self):
        conn = self._get_connection()
        try:
            yield conn
        finally:
            self._return_connection(conn)
    
    def _get_connection(self):
        with self.lock:
            if self.pool:
                return self.pool.pop()
            else:
                conn = sqlite3.connect(
                    self.db_path,
                    check_same_thread=False,
                    timeout=30.0
                )
                conn.row_factory = sqlite3.Row
                return conn
    
    def _return_connection(self, conn):
        with self.lock:
            if len(self.pool) < self.max_connections:
                self.pool.append(conn)
            else:
                conn.close()
```

### 5. **ë©”ëª¨ë¦¬ ìµœì í™”**

#### ì§€ì—° ë¡œë”©
```python
class LazyDataLoader:
    def __init__(self):
        self._clients = None
        self._care_areas = None
        self._event_types = None
    
    @property
    def clients(self):
        if self._clients is None:
            self._clients = self.load_clients()
        return self._clients
    
    @property
    def care_areas(self):
        if self._care_areas is None:
            self._care_areas = self.load_care_areas()
        return self._care_areas
    
    def clear_cache(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë¦¬"""
        self._clients = None
        self._care_areas = None
        self._event_types = None
```

#### í˜ì´ì§€ë„¤ì´ì…˜
```python
def get_clients_paginated(site, page=1, per_page=50, search_term=None):
    """í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    offset = (page - 1) * per_page
    
    query = """
        SELECT * FROM clients_cache 
        WHERE site = ? AND is_active = 1
    """
    params = [site]
    
    if search_term:
        query += " AND (client_name LIKE ? OR room_number LIKE ?)"
        params.extend([f"%{search_term}%", f"%{search_term}%"])
    
    query += " ORDER BY client_name LIMIT ? OFFSET ?"
    params.extend([per_page, offset])
    
    with get_db_connection() as conn:
        clients = conn.execute(query, params).fetchall()
        
        # ì „ì²´ ê°œìˆ˜ë„ í•¨ê»˜ ë°˜í™˜
        count_query = query.replace("SELECT *", "SELECT COUNT(*)").split("ORDER BY")[0]
        total = conn.execute(count_query, params[:-2]).fetchone()[0]
        
        return {
            'clients': [dict(c) for c in clients],
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': (total + per_page - 1) // per_page
        }
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •
```python
import time
import logging
from functools import wraps

def measure_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        execution_time = (end_time - start_time) * 1000  # ms
        logging.info(f"{func.__name__} ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ms")
        
        # ì„±ëŠ¥ ì„ê³„ê°’ ê²½ê³ 
        if execution_time > 100:  # 100ms ì´ìƒ
            logging.warning(f"{func.__name__} ì„±ëŠ¥ ì €í•˜: {execution_time:.2f}ms")
        
        return result
    return wrapper

@measure_performance
def get_clients_with_monitoring(site):
    return get_clients(site)
```

### ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§
```python
class CacheMetrics:
    def __init__(self):
        self.hits = 0
        self.misses = 0
    
    @property
    def hit_rate(self):
        total = self.hits + self.misses
        return (self.hits / total * 100) if total > 0 else 0
    
    def record_hit(self):
        self.hits += 1
    
    def record_miss(self):
        self.misses += 1
    
    def get_stats(self):
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': f"{self.hit_rate:.1f}%",
            'total_requests': self.hits + self.misses
        }
```

## ğŸ” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### 1. ë¶€í•˜ í…ŒìŠ¤íŠ¸
```python
import asyncio
import time

async def load_test_clients():
    """í´ë¼ì´ì–¸íŠ¸ ì¡°íšŒ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    sites = ['Parafield Gardens', 'Nerrilda', 'Ramsay', 'Yankalilla']
    
    start_time = time.time()
    
    # 100ê°œì˜ ë™ì‹œ ìš”ì²­
    tasks = []
    for i in range(100):
        site = sites[i % len(sites)]
        tasks.append(get_clients(site))
    
    results = await asyncio.gather(*tasks)
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"100ê°œ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {total_time/100*1000:.2f}ms")
    print(f"ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰: {100/total_time:.2f} req/sec")
```

### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
```python
import psutil
import os

def memory_usage_test():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
    process = psutil.Process(os.getpid())
    
    # ì‹œì‘ ë©”ëª¨ë¦¬
    start_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # ëŒ€ëŸ‰ ë°ì´í„° ë¡œë“œ
    all_clients = []
    for site in ['Parafield Gardens', 'Nerrilda', 'Ramsay', 'Yankalilla']:
        clients = get_clients(site)
        all_clients.extend(clients)
    
    # ì¢…ë£Œ ë©”ëª¨ë¦¬
    end_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    print(f"ì‹œì‘ ë©”ëª¨ë¦¬: {start_memory:.2f}MB")
    print(f"ì¢…ë£Œ ë©”ëª¨ë¦¬: {end_memory:.2f}MB")
    print(f"ë©”ëª¨ë¦¬ ì¦ê°€: {end_memory - start_memory:.2f}MB")
    print(f"í´ë¼ì´ì–¸íŠ¸ ìˆ˜: {len(all_clients)}")
```

## ğŸ¯ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
- [ ] ì ì ˆí•œ ì¸ë±ìŠ¤ ìƒì„±
- [ ] ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„
- [ ] ë¶ˆí•„ìš”í•œ ë°ì´í„° ì •ë¦¬
- [ ] VACUUM ì‹¤í–‰ìœ¼ë¡œ DB ìµœì í™”

### âœ… ìºì‹± ìµœì í™”
- [ ] ë‹¤ì¸µ ìºì‹± êµ¬í˜„
- [ ] ìºì‹œ ë§Œë£Œ ì •ì±… ì„¤ì •
- [ ] ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ

### âœ… ì½”ë“œ ìµœì í™”
- [ ] ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬í˜„
- [ ] ì—°ê²° í’€ë§ ì ìš©
- [ ] ì§€ì—° ë¡œë”© êµ¬í˜„
- [ ] í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©

### âœ… ëª¨ë‹ˆí„°ë§
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì •ê¸°ì  ì„±ëŠ¥ ë¦¬í¬íŠ¸

ì´ ìµœì í™” ì „ëµì„ í†µí•´ **JSON ê¸°ë°˜ ì‹œìŠ¤í…œ ëŒ€ë¹„ 10ë°° ì´ìƒì˜ ì„±ëŠ¥ í–¥ìƒ**ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
